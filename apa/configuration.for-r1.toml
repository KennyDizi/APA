temperature      = 0.2

# enable incremental output (true / false)
stream = true

programming_language = "Python"
thinking_tokens  = 16384         # anthropic only

# default model to send to LiteLLM (overrides provider-specific default)
provider = "openrouter" # Could be openai, anthropic, deepseek or openrouter
model = "deepseek/deepseek-r1-0528" # Should be o3, o4-mini, claude-sonnet-4-20250514, claude-opus-4-20250514

# fallback configuration
fallback_provider = "anthropic"
fallback_model = "claude-sonnet-4-20250514"
